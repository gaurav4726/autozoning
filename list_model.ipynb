{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing necessary libraries\n",
    "!pip install deepdoctection\n",
    "!pip install PyMUPDF rapidfuzz pdf2image pillow google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y tesseract-ocr\n",
    "!apt-get install -y poppler-utils\n",
    "# Restart runtime after installation\n",
    "# Runtime -> Restart runtime\n",
    "\n",
    "import os\n",
    "\n",
    "# Set the path to Tesseract's data directory\n",
    "os.environ['TESSDATA_PREFIX'] = '/usr/share/tesseract-ocr/4.00/tessdata'\n",
    "print(os.environ['PATH'])              # Verify the PATH\n",
    "\n",
    "# Set the Poppler directory in PATH\n",
    "os.environ['PATH'] += \":/usr/bin\"\n",
    "print(os.environ['PATH'])               # Verify the PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdoctection as dd\n",
    "from IPython.core.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import PyPDF2\n",
    "import json\n",
    "import random\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "analyzer = dd.get_dd_analyzer()  # instantiate the built-in analyzer similar to the Hugging Face space demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory where you want to save the JSON\n",
    "output_folder = '/content/output'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Open the complete PDF file\n",
    "pdf_file_path = '/content/output/page_15.pdf'\n",
    "pdf_file = open(pdf_file_path, 'rb')\n",
    "\n",
    "# Create a PDF reader object\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "# Iterate through each page and create a separate PDF for each page\n",
    "for page_num in range(len(pdf_reader.pages)):\n",
    "    # Create a new PDF writer object for each page\n",
    "    pdf_writer = PyPDF2.PdfWriter()\n",
    "\n",
    "    # Add the current page to the writer\n",
    "    pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "\n",
    "    # Create a new PDF file for the current page in the output folder\n",
    "    output_pdf_path = os.path.join(output_folder, f'page_{page_num + 1}.pdf')\n",
    "    with open(output_pdf_path, 'wb') as output_pdf:\n",
    "        pdf_writer.write(output_pdf)\n",
    "\n",
    "    #give pdf path of each page to model\n",
    "    pdf_path = output_pdf_path\n",
    "\n",
    "\n",
    "    #initialize the pdf path and variables required.\n",
    "    scores = []\n",
    "    ulx_val =[]\n",
    "    uly_val = []\n",
    "    lrx_val =[]\n",
    "    lry_val = []\n",
    "    heights = []\n",
    "    widths = []\n",
    "    areas = []\n",
    "    page_numbers = []\n",
    "    element_text = []\n",
    "    category_names = []\n",
    "\n",
    "    df = analyzer.analyze(path = pdf_path)  # setting up pipeline\n",
    "    df.reset_state()                 # Trigger some initialization\n",
    "\n",
    "    doc = iter(df)\n",
    "\n",
    "    page_number = 1\n",
    "    for page in doc:\n",
    "        image = page.viz()\n",
    "        plt.figure(figsize=(25, 17))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        print(page)\n",
    "        for annotation in page.annotations:\n",
    "        # Extract relevant information from the current annotation\n",
    "          category_name = annotation._category_name\n",
    "          score = annotation.score\n",
    "          height = annotation.bounding_box.height\n",
    "          width = annotation.bounding_box.width\n",
    "          area = height * width\n",
    "          ulx = annotation.bounding_box.ulx\n",
    "          uly = annotation.bounding_box.uly\n",
    "          lrx = annotation.bounding_box.lrx\n",
    "          lry = annotation.bounding_box.lry\n",
    "          #condition used to extract list and text only\n",
    "          # if (category_name == 'list' and area > 120000 and score >= 0.4) |(category_name == 'text') |(category_name == 'title'):\n",
    "          if (category_name == 'list' and area > 120000 and score >= 0.4):\n",
    "            element_text.append(annotation.text)\n",
    "            page_numbers.append(page_number)\n",
    "\n",
    "\n",
    "          # Append extracted information to lists\n",
    "          category_names.append(category_name)\n",
    "          scores.append(score)\n",
    "          heights.append(height)\n",
    "          widths.append(width)\n",
    "          areas.append(area)\n",
    "          ulx_val.append(ulx)\n",
    "          uly_val.append(uly)\n",
    "          lrx_val.append(lrx)\n",
    "          lry_val.append(lry)\n",
    "        page_number +=1\n",
    "\n",
    "    data = {\n",
    "        \"category_name\": category_names,\n",
    "        \"score\": scores,\n",
    "        \"Height\" : heights,\n",
    "        \"Width\" : widths,\n",
    "        \"area\" : areas,\n",
    "        \"Left\" : ulx_val,\n",
    "        \"Top\" : uly_val,\n",
    "        \"lrx\" : lrx_val,\n",
    "        \"lry\" : lry_val,\n",
    "    }\n",
    "\n",
    "    #adding the information of all the annotations to a dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "    #getting the details of only list and text from df and saving in another df.\n",
    "    element_df = df[((df['category_name'] == 'list') & (df['area'] >= 120000)) & (df['score'] >= 0.4)]\n",
    "    element_df = element_df[['category_name', 'score','Height','Width','Left','Top','lrx','lry']]\n",
    "    element_df['text'] = element_text\n",
    "    element_df['Page No'] = page_numbers\n",
    "\n",
    "    for image_id, data in page.embeddings.items():\n",
    "        original_image_height = data.height\n",
    "        original_image_width = data.width\n",
    "pdf_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_df = element_df[['Page No', 'Left', 'Top', 'Width', 'Height']]\n",
    "print(element_df)\n",
    "element_df[['New Left', 'New Top', 'New Width', 'New Height']] = element_df.apply(\n",
    "    lambda row: manipulate_coordinates(row['Left'], row['Top'], row['Width'], row['Height'], original_image_height, original_image_width),\n",
    "    axis=1,\n",
    "    result_type='expand'\n",
    ")\n",
    "\n",
    "# Select the desired columns\n",
    "new_df = element_df[['Page No', 'New Left', 'New Top', 'New Width', 'New Height']]\n",
    "\n",
    "# Print the new DataFrame\n",
    "new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate_coordinates(left,top,width,height,page_height,page_width):\n",
    "    # Scale coordinates for the new image size (825x1088)\n",
    "    scale_factor_x = 1023 / page_width  #\n",
    "    scale_factor_y = 825 / page_height  #\n",
    "    print(scale_factor_x)\n",
    "    new_left = (left * scale_factor_x) - 5\n",
    "    new_top = (top * scale_factor_y) - 5\n",
    "    new_height = (height * scale_factor_y) + 6\n",
    "    new_width = (width * scale_factor_x ) + 6\n",
    "\n",
    "    return new_left, new_top, new_height, new_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the single-page PDF\n",
    "pdf_file_path = '/content/output/page_2.pdf'\n",
    "pdf_document = fitz.open(pdf_file_path)\n",
    "\n",
    "# Select the page (assuming it's the first page)\n",
    "page = pdf_document[0]\n",
    "\n",
    "# Convert the page to an image\n",
    "pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))\n",
    "\n",
    "# Convert the image to a NumPy array\n",
    "image_data = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.h, pix.w, 3)\n",
    "\n",
    "# Load the element data from your DataFrame (element_df)\n",
    "# Assuming you have a DataFrame with columns: 'Left', 'Top', 'lrx', and 'lry'\n",
    "# Replace this with your actual DataFrame and column names\n",
    "# element_df = pd.read_csv('your_element_data.csv')\n",
    "\n",
    "# Extract the coordinates from the DataFrame\n",
    "x1_values = element_df['Left'].astype(int).tolist()\n",
    "y1_values = element_df['Top'].astype(int).tolist()\n",
    "x2_values = element_df['lrx'].astype(int).tolist()\n",
    "y2_values = element_df['lry'].astype(int).tolist()\n",
    "\n",
    "# Create a mask with the same size as the image\n",
    "mask = np.zeros_like(image_data)\n",
    "\n",
    "# Apply the masks for each set of coordinates\n",
    "for x1, y1, x2, y2 in zip(x1_values, y1_values, x2_values, y2_values):\n",
    "    mask[y1:y2, x1:x2, :] = 255  # Set the region within the coordinates to white\n",
    "\n",
    "# Apply the final mask to the image\n",
    "masked_image = cv2.bitwise_and(image_data, mask)\n",
    "\n",
    "# Save the resulting image as a new image (e.g., JPEG)\n",
    "cv2.imwrite('masked_image.jpg', masked_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q \"/content/image-to-livetext-1 (2).zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import os\n",
    "    import json\n",
    "    from google.cloud import vision\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    # Set up credentials and environment\n",
    "    credentials_path = r'/content/image-to-livetext-1/credentials.json'\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "\n",
    "\n",
    "    def detect_text(image_path):\n",
    "        # Reads the image file\n",
    "        with open(image_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        image = vision.Image(content=content)\n",
    "\n",
    "        # Performs text detection on the image file\n",
    "        response = client.text_detection(image=image)\n",
    "\n",
    "        fullTextAnnotation = response.full_text_annotation\n",
    "        json_data = vision.TextAnnotation.to_json(fullTextAnnotation)\n",
    "\n",
    "        return json_data\n",
    "\n",
    "    import json\n",
    "    import os\n",
    "    from jinja2 import Environment, FileSystemLoader\n",
    "    from PIL import Image\n",
    "\n",
    "\n",
    "    def get_image_size(image_path):\n",
    "          with Image.open(image_path) as img:\n",
    "              return img.size\n",
    "\n",
    "    def process_images_in_folder(folder_path):\n",
    "          # Create a folder for JSON files\n",
    "          parent_folder = os.path.dirname(folder_path)\n",
    "          json_folder = os.path.join(parent_folder, 'jsons')\n",
    "          html_folder = os.path.join(parent_folder, 'htmls')\n",
    "          os.makedirs(json_folder, exist_ok=True)\n",
    "          os.makedirs(html_folder, exist_ok=True)\n",
    "\n",
    "          # Iterate over the images in the folder\n",
    "          for filename in os.listdir(folder_path):\n",
    "              if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                  image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                  # Check if a corresponding JSON file already exists\n",
    "                  response_file = os.path.splitext(filename)[0] + '.json'\n",
    "                  response_path = os.path.join(json_folder, response_file)\n",
    "                  json_exists = os.path.exists(response_path)\n",
    "\n",
    "                  if not json_exists:\n",
    "                      text_annotation = detect_text(image_path)\n",
    "                      # print(text_annotation)\n",
    "\n",
    "                      with open(response_path, 'w', encoding='utf-8') as fle:\n",
    "                          fle.write(text_annotation)\n",
    "\n",
    "                  else:\n",
    "                      # Read the contents of the existing JSON file\n",
    "                      with open(response_path, 'r') as fle:\n",
    "                          text_annotation = fle.read() # json.load(fle)\n",
    "                          # print(text_annotation)\n",
    "\n",
    "    def main():\n",
    "      folder_path = '/content/image-to-livetext-1/data/image'\n",
    "      process_images_in_folder(folder_path)\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "\n",
    "    # Specify the folder containing the JSON files\n",
    "    file_path = f\"/content/image-to-livetext-1/data/jsons\"\n",
    "\n",
    "    # Function to extract text from JSON element\n",
    "    def extract_text(element):\n",
    "        if \"text\" in element:\n",
    "            return element[\"text\"]\n",
    "        elif \"symbols\" in element:\n",
    "            return \"\".join([symbol[\"text\"] for symbol in element[\"symbols\"]])\n",
    "        elif \"words\" in element:\n",
    "            return \" \".join([extract_text(word) for word in element[\"words\"]])\n",
    "        elif \"paragraphs\" in element:\n",
    "            return \"\\n\".join([extract_text(paragraph) for paragraph in element[\"paragraphs\"]])\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    # Create lists to store the data\n",
    "    #file_list = []\n",
    "    text_list = []\n",
    "    left_list = []\n",
    "    width_list = []\n",
    "    height_list = []\n",
    "    top_list = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "            # Extract text, paragraphs, and their bounding boxes\n",
    "        text = data[\"text\"]\n",
    "        paragraphs = []\n",
    "\n",
    "        for page in data[\"pages\"]:\n",
    "                for block in page[\"blocks\"]:\n",
    "                    if \"paragraphs\" in block:\n",
    "                        for paragraph in block[\"paragraphs\"]:\n",
    "                            paragraph_text = extract_text(paragraph)\n",
    "                            paragraph_bbox = paragraph[\"boundingBox\"][\"vertices\"]\n",
    "\n",
    "                            # Extract coordinates\n",
    "                            left, top = paragraph_bbox[0]['x'], paragraph_bbox[0]['y']\n",
    "                            width = paragraph_bbox[2]['x'] - left\n",
    "                            height = paragraph_bbox[2]['y'] - top\n",
    "\n",
    "                            # Append data to the lists\n",
    "                            #file_list.append(filename)\n",
    "                            text_list.append(paragraph_text)\n",
    "                            left_list.append(left)\n",
    "                            top_list.append(top)\n",
    "                            width_list.append(width)\n",
    "                            height_list.append(height)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    para_df = pd.DataFrame({\n",
    "        #\"File\": file_list,\n",
    "        \"text\": text_list,\n",
    "        \"Left\": left_list,\n",
    "        \"Top\": top_list,\n",
    "        \"Width\": width_list,\n",
    "        \"Height\": height_list\n",
    "    })\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
